# MLP B√°sica para Classifica√ß√£o de Estrelas Pulsares

Este reposit√≥rio apresenta a constru√ß√£o de uma rede neural **Multi-Layer Perceptron (MLP)** a partir do zero, utilizando apenas a biblioteca **NumPy** para todas as opera√ß√µes de √°lgebra linear e o processo de treinamento. O objetivo √© classificar poss√≠veis estrelas pulsares.

Este projeto foi desenvolvido como um exerc√≠cio pr√°tico para aprofundar o entendimento sobre os mecanismos fundamentais de uma rede neural, sem a abstra√ß√£o de frameworks como TensorFlow ou PyTorch.

Ele foi feito durante a disciplina de Computa√ß√£o Natural por mim e 2 colegas de classe.



---

## üß† Funcionamento da Rede Neural

A implementa√ß√£o cobre todas as etapas essenciais do funcionamento de uma MLP:

1.  **Estrutura da Rede**: A arquitetura da rede √© definida com uma camada de entrada, uma camada oculta e uma camada de sa√≠da. Os pesos e biases s√£o inicializados aleatoriamente.

2.  **Forward Propagation (Etapa de Predi√ß√£o)**:
    * Os dados de entrada (poss√≠veis estrelas) s√£o "achatados" (flattened) em um vetor.
    * A entrada √© multiplicada pelos pesos da primeira camada, e o bias √© somado.
    * A fun√ß√£o de ativa√ß√£o **Tangente Hiperb√≥lica** √© aplicada ao resultado.
    * O processo se repete para a camada de sa√≠da, que utiliza a **fun√ß√£o Sigm√≥ide** para converter os scores em probabilidades para uma das duas classes.

3.  **Backpropagation e Otimiza√ß√£o (Etapa de Treinamento)**:
    * O erro entre a predi√ß√£o (sa√≠da da Sigm√≥ide) e o r√≥tulo verdadeiro √© calculado usando a fun√ß√£o de perda **Binary Cross Entropy**.
    * O algoritmo de **backpropagation** √© implementado para calcular o gradiente da fun√ß√£o de perda em rela√ß√£o a cada peso e bias da rede.
    * Os pesos e biases s√£o atualizados na dire√ß√£o oposta ao gradiente, aplicando o algoritmo de otimiza√ß√£o **Gradiente Descendente**, com uma taxa de aprendizado (`learning rate`) definida.

O ciclo de *forward propagation*, c√°lculo do erro e *backpropagation* √© repetido por v√°rias √©pocas (itera√ß√µes sobre todo o conjunto de dados de treino) para minimizar o erro e "ensinar" a rede a classificar os d√≠gitos corretamente.

---

## üìä Resultados de Desempenho

A performance do modelo foi avaliada no conjunto de teste do dataset. Os resultados obtidos foram os seguintes:

| M√©trica         | Valor |
| :-------------- | :---: |
| Acur√°cia        |   97%  |
| Precis√£o (M√©dia) |   94% |
| Recall (M√©dia)   |   69% |
| F1-Score (M√©dia) |   80% |

---

## Autores

- [@GabrielMSchmidt](https://www.github.com/GabrielMSchmidt)
- [@GabrielNLima](https://www.github.com/GabrielNLima)
- [@KWSantos](https://www.github.com/KWSantos)

